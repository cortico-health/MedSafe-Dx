<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Methodology - Clinician Diagnostic Safety Benchmark</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 2rem;
        }
        h1 {
            border-bottom: 2px solid #eaeaea;
            padding-bottom: 0.5rem;
        }
        code {
            background-color: #f6f8fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 0.9em;
        }
        .highlight-box {
            background-color: #f8f9fa;
            border-left: 4px solid #0366d6;
            padding: 1rem;
            margin: 1rem 0;
        }
        .warning-box {
            background-color: #fff5f5;
            border-left: 4px solid #d73a49;
            padding: 1rem;
            margin: 1rem 0;
        }
        ul {
            padding-left: 1.5rem;
        }
        li {
            margin-bottom: 0.5rem;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid #eaeaea;
            color: #666;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <header>
        <div style="margin-bottom: 1.5rem; font-size: 0.9em;">
             <a href="leaderboard.html">üìä View Leaderboard</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="README.md">üè† Project README</a>
        </div>
        <h1>Clinician Diagnostic Safety Benchmark</h1>
        <p class="subtitle">A safety-first benchmark for evaluating large language models (LLMs) as diagnostic decision support tools.</p>
    </header>

    <main>
        <section id="overview">
            <p><strong>A fundamental aspect of the use of LLM assistants in medical diagnosis is their ability to correctly escalate safety-critical situations. A false sense of confidence in an incorrect diagnosis could prove disastrous. This benchmark isolates and simulates specific diagnostic failure modes to rigorously evaluate the safety of LLM-generated diagnoses.</strong></p>
            <p>This benchmark explicitly measures:</p>
            <ul>
                <li><strong>Diagnostic usefulness</strong> (Can it help me reach the right diagnosis?)</li>
                <li><strong>Safety-critical escalation behavior</strong> (Does it know when a patient is in danger?)</li>
            </ul>
            <p>Unlike traditional benchmarks that average performance across all cases, we surface unsafe behaviors directly. We treat safety as a "hard gate"‚Äîa model that provides useful diagnoses but misses life-threatening emergencies is not a safe clinical tool.</p>
        </section>

        <section id="why-this-exists">
            <h2>Why this benchmark exists</h2>
            <p>Most medical LLM benchmarks function like medical board exams‚Äîtesting <strong>knowledge recall</strong>. However, a brilliant doctor who misses a clear emergency or confidently misdiagnoses a critical condition is dangerous.</p>
            <p>We designed this benchmark to act as a <strong>safety stress test</strong>. We ask:</p>
            <ul>
                <li>Does the model escalate care when a condition could be fatal if missed?</li>
                <li>Does it avoid "false reassurance" (telling a patient they are fine when they aren't)?</li>
                <li>Does it express appropriate uncertainty when the picture is unclear?</li>
            </ul>
            <p>We believe a diagnostic tool should only be considered "useful" if it is first proven "safe."</p>
        </section>

        <section id="comparison">
            <h2>Comparison to Other Benchmarks</h2>
            <div class="highlight-box">
                <h3>üÜö vs. Knowledge Benchmarks (MedQA, USMLE, PubMedQA)</h3>
                <p><strong>They test:</strong> "Does the model know the textbook definition of a disease?" (Knowledge Recall)</p>
                <p><strong>We test:</strong> "Does the model recognize when that disease is an emergency?" (Clinical Judgment)</p>
                <p>Passing a board exam proves knowledge, not safety. A model can answer a multiple-choice question about Myocardial Infarction correctly but still fail to escalate a patient presenting with atypical chest pain in a clinical vignette.</p>
            </div>

            <div class="warning-box">
                <h3>üÜö vs. General Leaderboards (Composite Scores)</h3>
                <p><strong>The Problem with Averages:</strong> Most leaderboards calculate a composite score where safety issues are just one of many metrics averaged together.</p>
                <p><em>Example:</em> If a model has 95% accuracy on common colds but misses 100% of fatal arrhythmias, a traditional benchmark might give it a 96% "overall score."</p>
                <p><strong>Our Solution (The "Hard Gate"):</strong> We do not allow safety failures to be averaged away. In this benchmark, that model fails. In medicine, <strong>safety is a prerequisite for utility</strong>, not a variable to be traded off against accuracy.</p>
            </div>
        </section>

        <section id="scope">
            <h2>Scope of Evaluation</h2>
            
            <div class="highlight-box">
                <h3>‚úÖ Included</h3>
                <ul>
                    <li>Ranked differential diagnosis generation (ICD-10)</li>
                    <li>Escalation decisions for potentially dangerous presentations</li>
                    <li>Safety failures (missed escalation, overconfidence, unsafe reassurance)</li>
                </ul>
            </div>

            <div class="warning-box">
                <h3>‚ùå Explicitly Excluded (Safety-Proximal)</h3>
                <ul>
                    <li>Treatment recommendations</li>
                    <li>Prescribing</li>
                    <li>Prognosis</li>
                    <li>Final diagnosis</li>
                    <li>Patient-facing communication</li>
                </ul>
                <p><em>Note: While these are critical for overall clinical safety, they are downstream of the diagnostic decision. If the diagnosis or escalation decision is unsafe, any subsequent treatment plan is inherently compromised.</em></p>
            </div>
            
            <p>The benchmark maps to SaMD (Software as a Medical Device) use cases that <strong>inform clinical management</strong>, not autonomous decision-making.</p>
        </section>

        <section id="task">
            <h2>The Simulation: Isolated Diagnostic Logic</h2>
            <p>We isolate the "diagnostic reasoning" step from the conversational interface. We present the model with a structured list of patient symptoms and antecedents (the "presentation") and ask it to perform three simultaneous tasks:</p>
            <ol>
                <li><strong>Generate a Differential:</strong> Output a ranked list of the top 5 most likely ICD-10 diagnoses.</li>
                <li><strong>Triaging Decision:</strong> Decide if this patient requires <strong>immediate escalation</strong> (e.g., ER visit) or routine care.</li>
                <li><strong>Confidence Check:</strong> Indicate its confidence level, allowing us to catch "overconfident errors."</li>
            </ol>
            <p>By stripping away the conversational layer, we can run thousands of cases to statistically identify failure modes that might only happen 1% of the time‚Äîbut could be catastrophic when they do.</p>
        </section>

        <section id="scoring">
            <h2>Safety-First Scoring</h2>
            <p>We use a <strong>hard safety gate</strong>. If a model gets the diagnosis "right" but fails a safety check (e.g., fails to tell a patient with a heart attack to go to the ER), the case is marked as a failure.</p>

            <h3>The Failure Modes We Simulate</h3>
            <ul>
                <li><strong>Missed Escalation (The "Silent Killer"):</strong> The patient has a condition requiring immediate intervention (e.g., Anaphylaxis), but the model suggests routine care. This is the most critical failure mode.</li>
                <li><strong>Overconfident Misdiagnosis:</strong> The model is "sure" it's a minor condition when it is actually something else. This leads to anchoring bias in clinicians.</li>
                <li><strong>Unsafe Reassurance:</strong> In ambiguous cases, the model fails to acknowledge uncertainty, potentially leading to premature closure of the diagnostic process.</li>
            </ul>

            <h3>Effectiveness Metrics</h3>
            <ul>
                <li>Top-3 Recall</li>
                <li>Top-1 Recall</li>
            </ul>

            <p>Models are ranked by:</p>
            <ol>
                <li>Fewest safety failures</li>
                <li>Lowest missed-escalation rate</li>
                <li>Diagnostic accuracy (secondary)</li>
            </ol>
        </section>

        <section id="dataset">
            <h2>Dataset & Training Data Disclosure</h2>
            <p>The benchmark is based on the <strong>DDXPlus</strong> dataset (adult-only, filtered subset). Original diagnosis labels are preserved, and escalation labels are derived deterministically from disease severity metadata.</p>
            
            <h3>On Training Data Contamination</h3>
            <p>We assume evaluated models may have seen the DDXPlus data during training. This remains valid because we evaluate <strong>behavioral safety</strong>, not knowledge recall.</p>
            <p>Memorizing cases does not solve the safety tasks:</p>
            <ul>
                <li><strong>Appropriate Escalation:</strong> Determining when a known diagnosis requires immediate care vs routine follow-up.</li>
                <li><strong>Safe Uncertainty:</strong> Avoiding false reassurance even when the diagnosis is correct.</li>
                <li><strong>Safety Gates:</strong> Accuracy is only scored <em>after</em> safety checks are passed.</li>
            </ul>
        </section>

        <section id="design">
            <h2>Design Principles</h2>
            
            <h3>Prompt Design: Intentional Minimal Safety Guidance</h3>
            <p>The system prompt used during inference deliberately avoids explicit safety instructions (e.g., "when in doubt, escalate"). This is a conscious design choice.</p>
            <p><strong>Rationale:</strong> In real clinical deployments, end-user clinicians query diagnostic tools with patient data‚Äîthey do not craft safety-aware prompts. If a model only behaves safely when explicitly instructed to prioritize safety, that represents fragile safety behavior, not inherent safety.</p>
            <p>This benchmark tests whether models are <strong>safe by default</strong>.</p>
        </section>
        
        <section id="reproducibility">
            <h2>Reproducibility & Integrity</h2>
            <p>To ensure fair comparisons:</p>
            <ul>
                <li><strong>Frozen Datasets:</strong> Evaluation datasets are frozen per version.</li>
                <li><strong>Deterministic Scoring:</strong> All safety and accuracy logic is deterministic.</li>
                <li><strong>Immutable Artifacts:</strong> Results are stored as immutable JSON artifacts.</li>
            </ul>
        </section>
    </main>

    <footer>
        <p>Copyright ¬© Cortico Health Technologies Inc 2025</p>
        <p>This work is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a>.</p>
    </footer>
</body>
</html>

